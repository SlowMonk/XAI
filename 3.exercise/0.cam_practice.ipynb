{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://bob3rdnewbie.tistory.com/320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_mnist(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN NETWORK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_mnist, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # 3 x 128 x 128\n",
    "            nn.Conv2d(1, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # 32 x 128 x 128\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # 64 x 128 x 128\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # 64 x 64 x 64\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # 128 x 64 x 64\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # 256 x 64 x 64\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # 256 x 32 x 32\n",
    "            nn.Conv2d(256, 10, 3, 1, 1),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        # 256 x 32 x 32\n",
    "        self.avg_pool = nn.AvgPool2d(7)\n",
    "        # 256 x 1 x 1\n",
    "        self.classifier = nn.Linear(10, 10)\n",
    "        self.name='CNN_mnist'\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.conv(x)\n",
    "        flatten = self.avg_pool(features).view(features.size(0), -1)\n",
    "        output = self.classifier(flatten)\n",
    "        return output, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_mnist(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): LeakyReLU(negative_slope=0.2)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): LeakyReLU(negative_slope=0.2)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (avg_pool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
      "  (classifier): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CNN_mnist()\n",
    "net.load_state_dict(torch.load(\"/home/jake/Gits/AI college/XAI/2.problem/model_weights/pth/cnn_mnist.pth\"))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정방향 역방향 훅 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.MNIST(root='/mnt/3CE35B99003D727B/input/pytorch/data', train=True,\n",
    "                                                download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,trainloader,step):\n",
    "    #print(\"training....\")\n",
    "\n",
    "    for i , (images, targets) in enumerate(trainloader):\n",
    "        #print(images.shape)\n",
    "        self.net.train()\n",
    "\n",
    "        images, targets = images.to(self.device), targets.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        if self.net.name =='CNN_mnist':\n",
    "            outputs ,_ = self.net(images)\n",
    "        else:\n",
    "             outputs= self.net(images)\n",
    "        loss = self.criterion(outputs,targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epoch_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        self.total += targets.size(0)\n",
    "        self.correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print('Epoch [%d/%d], Iter [%d/%d], Loss: %.4f  correct: %.4f %%'\n",
    "                  % (epoch + 1, 10, i + 1, len(trainloader), loss.item(),100.*self.correct/self.total))\n",
    "\n",
    "    avg_epoch_loss = self.epoch_loss / len(trainloader)\n",
    "\n",
    "    print(\"Epoch: %d, Avg Loss: %.4f\" % (epoch + 1, avg_epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.rand(32,1,28,28)\n",
    "input = torch.tensor(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printnorm(input,output):\n",
    "    #print('inside' + self.__class__.name__+ 'forward')\n",
    "    print('')\n",
    "    print('input:', type(input))\n",
    "    print('input[0]', type(input[0]))\n",
    "    print('output:',type(output))\n",
    "    print('')\n",
    "    print('input size', input[0].size())\n",
    "    print('output size',output.data.size())\n",
    "    print('output norm',output.data.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.conv.register_forward_hook(printnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_mnist(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): LeakyReLU(negative_slope=0.2)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.2)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(256, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (classifier): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_blobs = []\n",
    "#net._modules.get(final_conv).register_forward_hook(hook_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('feature_blobs->',torch.tensor(feature_blobs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (28, 28)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    print('class_idx->',class_idx)\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h * w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_softmax-> (10, 10)\n",
      "0 torch.Size([1, 1, 28, 28])\n",
      "feature_blobs-> torch.Size([25, 1, 10, 7, 7])\n",
      "feature_blobstemp-> (1, 10, 7, 7)\n",
      "idx[0].item()-> [7]\n",
      "class_idx-> [7]\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "final_conv = 'conv'\n",
    "feature_blobs = []\n",
    "\n",
    "params = list(net.parameters())\n",
    "\n",
    "#print('params->',params[-2])\n",
    "\n",
    "weight_softmax = np.squeeze(params[-2].cpu().data.numpy())\n",
    "\n",
    "print('weight_softmax->',weight_softmax.shape)\n",
    "result\n",
    "def hook_feature(module, input, output):\n",
    "    feature_blobs.append(output.cpu().data.numpy())\n",
    "\n",
    "for i , (img, targets) in enumerate(train_loader):\n",
    "    print(i,img.shape)\n",
    "    net._modules.get(final_conv).register_forward_hook(hook_feature)\n",
    "    img, targets = img.to(device), targets.to(device)\n",
    "    logit ,_ = net(images)\n",
    "    \n",
    "    x = img[0, :, :].cpu().data.numpy().squeeze()\n",
    "    #feature_blobs.append(outputs.cpu().data.numpy())\n",
    "    print('feature_blobs->',torch.tensor(feature_blobs).shape)\n",
    "    \n",
    "    #print('logit->',logit)\n",
    "    h_x = F.softmax(logit,dim=1).data.squeeze()\n",
    "    #print('h_x->',h_x)\n",
    "    \n",
    "    weight_softmaxtemp = weight_softmax\n",
    "    feature_blobstemp = feature_blobs[0]\n",
    "    print('feature_blobstemp->',feature_blobstemp.shape)\n",
    "    \n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    #print('probs->',probs,'idx->',idx)\n",
    "    print('idx[0].item()->',[idx[0].item()])\n",
    "    \n",
    "    CAMs = returnCAM(feature_blobs[0], weight_softmax, [idx[0].item()])\n",
    "    \n",
    "    height, width = img.shape[2:]\n",
    "\n",
    "    CAM = cv2.resize(CAMs[0], (width, height))\n",
    "    heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "    result = (x * CAM)\n",
    "\n",
    "    \n",
    "    if i==0:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape\n",
    "img = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake/venv/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = img.squeeze()\n",
    "img = torch.tensor(img)\n",
    "print(img.shape)\n",
    "#img = (img).permute(1, 2, 0)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
