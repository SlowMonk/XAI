{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "#from customFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import utils\n",
    "import model\n",
    "from attribution_methods_mnist import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from evaluaion_methods import *\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============cam_mnist============\n",
      "LOAD DATA, 60000\n",
      "LOAD DATA, 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"==============cam_mnist============\")\n",
    "\n",
    "if not os.path.exists('./result'):\n",
    "    os.mkdir('result/')\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "net = model.load_net().to(device)\n",
    "finalconv_name = 'conv'\n",
    "save_dir = \"../datab\"\n",
    "ratio = 0.1\n",
    "\n",
    "train_loader_mnist = utils.load_data_mnist(batch_size=1,test=False)\n",
    "test_loader_mnist = utils.load_data_mnist(batch_size=1, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_mnist.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    #npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(img, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "        #plt.imshow(np.transpose(img, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "img=train_loader_mnist.dataset.data[0]\n",
    "img = img.detach().numpy()\n",
    "img = cv2.applyColorMap(img,cv2.COLORMAP_JET)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib_imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model.load__MnistCNN().to(device)\n",
    "features_blobs = []\n",
    "finalconv_name = 'conv'\n",
    "final_conv = 'conv'\n",
    "net.eval()\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].cpu().data.numpy())\n",
    "device = device\n",
    "datapath = \"./datab/file_mnist2.hdf5\"\n",
    "i = 0\n",
    "dset = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_mnist(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Conv2d(28, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): LeakyReLU(negative_slope=0.2)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.2)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(256, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=28, stride=28, padding=0)\n",
       "  (classifier): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fb08dccfa20>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "net._modules.get(final_conv).register_forward_hook(hook_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.cpu().data.numpy())\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "features_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam(net, features_blobs, img_pil, root_img):\n",
    "    print('get_cam')\n",
    "    params = list(net.parameters())\n",
    "    weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    img_tensor = preprocess(img_pil)\n",
    "    img_variable = Variable(img_tensor.unsqueeze(0)).cuda()\n",
    "    logit = net(img_variable)\n",
    "\n",
    "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "\n",
    "    # output: the prediction\n",
    "    #for i in range(0, 2):\n",
    "    #    line = '{:.3f} -> {}'.format(probs[i], classes[idx[i].item()])\n",
    "    #    print(line)\n",
    "\n",
    "    CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0].item()])\n",
    "\n",
    "    # render the CAM and output\n",
    "    #print('output CAM.jpg for the top1 prediction: %s' % classes[idx[0].item()])\n",
    "    img = cv2.imread(root_img)\n",
    "    height, width, _ = img.shape\n",
    "    CAM = cv2.resize(CAMs[0], (width, height))\n",
    "    heatmap = cv2.applyColorMap(CAM, cv2.COLORMAP_JET)\n",
    "    result = heatmap * 0.3 + img * 0.5\n",
    "    cv2.imwrite('cam.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_cam\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size 28 1 3 3, expected input[1, 3, 224, 224] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-ad4ec313f41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtransToTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransToPil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mget_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_blobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#cv2.imwrite('./result/cam{}.jpg'.format(idx), heatmap_b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-254-a5955b8176bd>\u001b[0m in \u001b[0;36mget_cam\u001b[0;34m(net, features_blobs, img_pil, root_img)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_pil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimg_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mh_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Gits/AI college/XAI/2.problem/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mflatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size 28 1 3 3, expected input[1, 3, 224, 224] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "for idx ,(img,target) in enumerate(train_loader_mnist):\n",
    "    root='./result/cam0.jpg'\n",
    "    transToPil = transforms.ToPILImage()\n",
    "    transToTensor = transforms.ToTensor()\n",
    "    #img = transToPil(torchvision.utils.make_grid(img))\n",
    "    get_cam(net,feature_blobs,img,root)\n",
    "    #cv2.imwrite('./result/cam{}.jpg'.format(idx), heatmap_b)\n",
    "    if idx ==1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    " def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "        size_upsample = (28, 28)\n",
    "        bz, nc, h, w = feature_conv.shape\n",
    "        output_cam = []\n",
    "        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h * w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "        #del cam_img,cam\n",
    "        #gc.collect()\n",
    "\n",
    "        #return output_cam\n",
    "        return cv2.resize(cam_img, size_upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-c1e0bc98ecde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCAMs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturnCAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_blobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_softmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(img, i):\n",
    "    global feature_blobs\n",
    "    print(img.shape)\n",
    "    print(\"=================generate image=============================\")\n",
    "    img = Variable(img, requires_grad=True)\n",
    "\n",
    "    net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "    img_tensor = img.to(device)\n",
    "    logit, _ = net(img_tensor)\n",
    "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "\n",
    "    weight_softmaxtemp = weight_softmax\n",
    "    feature_blobstemp = feature_blobs[0]\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    idx_temp = [idx[0]]\n",
    "    output_cam = returnCAM(feature_blobs[0], weight_softmax, [idx[0].item()])\n",
    "    height, width = 28, 28\n",
    "    heatmap = cv2.applyColorMap(cv2.resize(output_cam[0], (width, height)), cv2.COLORMAP_JET)\n",
    "    #heatmap2 = cv2.resize(output_cam[0], (width, height))\n",
    "\n",
    "    img = img.detach().numpy()\n",
    "    img2 = img[0]\n",
    "    img2 = np.transpose(img2, axes=(1, 2, 0))\n",
    "    img2 = cv2.resize(img2, (28, 28))\n",
    "    img2 = cv2.applyColorMap(cv2.resize(output_cam[0], (width, height)), cv2.COLORMAP_JET)\n",
    "\n",
    "    camsresult = np.array(list(map(resize_image, heatmap, img2)))\n",
    "    result = heatmap* 0.3 + img2 * 0.7\n",
    "\n",
    "    \n",
    "    feature_blobs = []\n",
    "    del img_tensor, output_cam, img2, img\n",
    "    gc.collect()\n",
    "    # result=zip(camsresult,heatmap,probs.detach().cpu().numpy(), idx.detach().cpu().numpy())\n",
    "\n",
    "    return camsresult,heatmap, probs.detach().cpu().numpy(), idx.detach().cpu().numpy(), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img/2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg),(1,2,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n",
      "=================generate image=============================\n",
      "torch.Size([1, 1, 128, 128])\n",
      "=================generate image=============================\n"
     ]
    }
   ],
   "source": [
    "for idx ,(img,target) in enumerate(train_loader_mnist):\n",
    "    sal_maps_b, heatmap_b,probs_b, preds_b,result=generate_image(img,idx)\n",
    "    cv2.imwrite('./result/cam{}.jpg'.format(idx), heatmap_b)\n",
    "    if idx ==1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_mnist.dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img=train_loader_mnist.dataset[1][0]\n",
    "#img.resize((1,) + img.shape)\n",
    "#print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sal_maps_b, probs_b, preds_b = generate_image(img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD DATA, 60000\n",
      "LOAD DATA, 10000\n",
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([3, 128, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import string\n",
    "import math\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_loader_mnist = utils.load_data_mnist(batch_size=1,test=False)\n",
    "test_loader_mnist = utils.load_data_mnist(batch_size=1, test=True)\n",
    "\n",
    "dataiter = iter(train_loader_mnist)\n",
    "images = dataiter.next()\n",
    "print(images[0].shape)\n",
    "print(torchvision.utils.make_grid(images[0]).shape)\n",
    "\n",
    "# show images\n",
    "# print(images[0])\n",
    "imshow(torchvision.utils.make_grid(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
